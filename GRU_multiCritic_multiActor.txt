ğŸ§  GRU-Enabled Multi-Critic Multi-Actor PPO with GAT Tutorial

This tutorial guides the implementation of a two-actor, two-critic PPO architecture enhanced with a GRU-based encoder and Graph Attention Network (GAT) for critic value aggregation. It supports dynamic UE count per batch and aligns with energy-efficient O-RAN slicing and scheduling.

ğŸ“¥ 1. Input: MAC + KPM Metrics

Each UE provides 17 features: 10 MAC + 7 KPM metrics.
Input shape: [batch_size, user_num, 17].
To support varying user_num across batches, pad input to [batch_size, max_user_num, 17].
Use a mask tensor: 1 for valid UEs, 0 for padded entries.

ğŸ”„ 2. GRU-Based State Encoder

- Architecture: 2-layer GRU
- Hidden sizes: first layer 128 â†’ second layer 64
- Output: [batch_size, 64] (final hidden state of top layer)
Masked padding is ignored using packing or masking techniques during training.

ğŸ›ï¸ 3. PPO Architecture
ğŸ”¸ Critics

1. QoS-Aware Critic:
   - Trained with slice-based reward:
     r_qos = - Î»_p * normalized QoS violations (throughput) - Î»_d * normalized QoS violations (delay)
2. Energy-Aware Critic:
   - Trained with energy efficiency reward:
     r_energy = normalized sleep_duration b_t
Both return scalar values: V_qos(s), V_energy(s)

ğŸ”¸ GAT Aggregation

- Inputs: [batch_size, num_critics=2, 1] value estimates from both critics
- Output: [batch_size, num_actors=2, 1] aggregated critic values for both actors, respectively
- Purpose: Enables each actor to get a context-sensitive value estimate

ğŸ”¸ Actors

1. Resource Slicing Actor:
   - Output: mean (mu) and std (sigma) for Gaussian distributions of slice percentages
   - Output dim: [batch_size, num_slices]
   - Sampled values normalized to sum to 100

2. Time Scheduling Actor:
   - Output: logits for categorical distribution over values a_t, b_t, c_t
   - Sampled as discrete subframe allocations summing to 10 (ms frame)

ğŸ“ˆ 4. PPO Training Flow
set num_slices = 3, for embb, urllc, mmtc slices with 3 sets of QoS requirement on delay and throughput
1. Compute critic values: [V_qos, V_energy]
2. Apply GAT â†’ [V_slicing, V_scheduling]
3. Compute returns for each critic using their respective rewards
4. Compute GAE advantages
5. Update actor using clipped PPO loss with GAE advantages
6. Update each critic using huber loss

ğŸ§ª 5. Feasibility Check

âœ… Dynamic user handling via padding + masking
âœ… GRU processes variable UE count into fixed embedding
âœ… Modular critics trained on separate reward functions
âœ… GAT cleanly maps critics â†’ actors
âœ… Actor design supports both continuous and discrete decisions
âœ… Advantage computation grounded in GAT-aggregated values
The design is consistent, scalable, and well-aligned with PPO + O-RAN control.
