🧠 GRU-Enabled Multi-Critic Multi-Actor PPO with GAT Tutorial

This tutorial guides the implementation of a two-actor, two-critic PPO architecture enhanced with a GRU-based encoder and Graph Attention Network (GAT) for critic value aggregation. It supports dynamic UE count per batch and aligns with energy-efficient O-RAN slicing and scheduling.

📥 1. Input: MAC + KPM Metrics

Each UE provides 17 features: 10 MAC + 7 KPM metrics.
Input shape: [batch_size, user_num, 17].
To support varying user_num across batches, pad input to [batch_size, max_user_num, 17].
Use a mask tensor: 1 for valid UEs, 0 for padded entries.

🔄 2. GRU-Based State Encoder

- Architecture: 2-layer GRU
- Hidden sizes: first layer 128 → second layer 64
- Output: [batch_size, 64] (final hidden state of top layer)
Masked padding is ignored using packing or masking techniques during training.

🏛️ 3. PPO Architecture
🔸 Critics

1. QoS-Aware Critic:
   - Trained with slice-based reward:
     r_qos = - λ_p * normalized QoS violations (throughput) - λ_d * normalized QoS violations (delay)
2. Energy-Aware Critic:
   - Trained with energy efficiency reward:
     r_energy = normalized sleep_duration b_t
Both return scalar values: V_qos(s), V_energy(s)

🔸 GAT Aggregation

- Inputs: [batch_size, num_critics=2, 1] value estimates from both critics
- Output: [batch_size, num_actors=2, 1] aggregated critic values for both actors, respectively
- Purpose: Enables each actor to get a context-sensitive value estimate

🔸 Actors

1. Resource Slicing Actor:
   - Output: mean (mu) and std (sigma) for Gaussian distributions of slice percentages
   - Output dim: [batch_size, num_slices]
   - Sampled values normalized to sum to 100

2. Time Scheduling Actor:
   - Output: logits for categorical distribution over values a_t, b_t, c_t
   - Sampled as discrete subframe allocations summing to 10 (ms frame)

📈 4. PPO Training Flow
set num_slices = 3, for embb, urllc, mmtc slices with 3 sets of QoS requirement on delay and throughput
1. Compute critic values: [V_qos, V_energy]
2. Apply GAT → [V_slicing, V_scheduling]
3. Compute returns for each critic using their respective rewards
4. Compute GAE advantages
5. Update actor using clipped PPO loss with GAE advantages
6. Update each critic using huber loss

🧪 5. Feasibility Check

✅ Dynamic user handling via padding + masking
✅ GRU processes variable UE count into fixed embedding
✅ Modular critics trained on separate reward functions
✅ GAT cleanly maps critics → actors
✅ Actor design supports both continuous and discrete decisions
✅ Advantage computation grounded in GAT-aggregated values
The design is consistent, scalable, and well-aligned with PPO + O-RAN control.
